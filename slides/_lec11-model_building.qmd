---
title: "Model Building" 
date: "2022-11-28"
description: "14"
author: "Robin Donatello"
footer: "[ðŸ”— https://math615.netlify.app](https://math615.netlify.app) / Model Building"
from: markdown+emoji
format: 
  revealjs:
    theme: beige
    transition: fade
    slide-number: true
    incremental: false
    code-fold: true
    code-summary: "Show the code"
    chalkboard: true
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:
      width: 200
---


## References

* PMA6 Chapter 9, 10 and 12.8
* ASCN Chapter 10 and 12.  


## Linear regression model refinements

# Identifying outliers (PMA6 Ch 7.8)


## Outliers in X

Possible outliers in $X$ are measured using a statistic called _leverage_, denoted by ($h$). 

$$
h = \frac{1}{N} \frac{(X - \bar{X})^2}{\sum(X - \bar{X})^2}
$$

When $X$ is far away from $\bar{X}$, the leverage is large. Observations with large leverage posses the potential for having a large effect on the slope of the line. 

## Influential Observations

Just looking at outliers in Y or X by themselves is not helpful. Instead you must look at the combination of the two to determine how much influence a single point may have. Ref. Figure 7.8 in PMA6. 




## What to do about outliers? 

* If an outlier is found then it is important to identify the case/record/observation that contains the suspected value. 
* Sometimes this process provides a clue as to the nature of the outlier. Then the options are: 
    * Removal of the outlier (just the single point)
    * Correct the record (if the outlier is due to a data entry error)
    * Removal of the entire row/observation

:::{.callout-warning}
Outliers should not be removed just because they represent outliers. Any decisions regarding outliers should be well justified and made transparent in any report or manuscript. 
:::

You should always fit the model with, and without the outlier to assess the true impact of that single record. 




## Model Fit

* ANOVA for Regression. How different is our fitted line $\hat{Y}$ from the overall mean $\bar{Y}$? The residual mean squared error (RMSE) is a measure of how poorly or well the regression line fits the actual data points. A large RMSE indicates a poor fit. 
    - While PMA6 and ASCN chapters 7.8 discuss ANOVA for regression in context of a simple linear regression, this generalizes to multiple regression. It tests all $\beta_p$ are simultaneously equal to 0. 


# Classification

```{r}
depress <- read.delim(here::here("data/Depress.txt"))
names(depress) <- tolower(names(depress))
depress$sex <- factor(depress$sex, labels = c("Male", "Female"))
dep_sex_model <- glm(cases ~ age + income + sex, data=depress, family="binomial")
phat.depr <- predict(dep_sex_model, type='response') 
model.pred.data <- cbind(dep_sex_model$data, phat.depr)
set.seed(12345) #reminder: change the combo on my luggage
plot.mpp <- data.frame(pred.prob = phat.depr, 
                       pred.class = rbinom(n = length(phat.depr), 
                                           size = 1, 
                                           p = phat.depr),
                       truth = dep_sex_model$y)
plot.mpp <- plot.mpp %>% 
            mutate(pred.class = factor(pred.class, labels=c("Not Depressed", "Depressed")), 
                    truth = factor(truth, labels=c("Not Depressed", "Depressed")))
```


## Prediction cutoff {.smaller}

```{r}
table(plot.mpp$pred.class, plot.mpp$truth)
```

This table was generated by drawing a random Bernoulli variable with probability $p_{i}$. This assumes that probabilities can range from \[0,1\], but if you recall, the predicted probabilities max out around 0.5.

Often we adjust the cutoff value to improve accuracy. This is where we have to put our gut feeling of what probability constitutes "high risk".

## Prediction cutoff {.smaller}

For some models, this could be as low as 30%. It's whatever the probability is that optimally separates the classes. This is an important tuning parameter because since the models we build are only based on data we measured, often there are other unmeasured confounding factors that affect the predicted probability. So our predictions don't span the full range from \[0,1\].

Using the above plots, where should we put the cutoff value? At what probability should we classify a record as "depressed"?

There are many different types of criteria that can be used to find the optimal cutoff value. But first we need to understand the expanded borders of a [\[Confusion Matrix\]](https://en.wikipedia.org/wiki/Confusion_matrix). Using the `confusionMatrix` function inside the `caret` package performs all these calculations for us.


::: {.callout-caution title="Handling missing data in the model"}
This is also another place where the factor ordering of binary variables can cause headache. Another reason to control your factors!
:::

```{r}
confusionMatrix(plot.mpp$pred.class, plot.mpp$truth, positive="Depressed")
```

-   195 people were correctly predicted to not be depressed (True
    Negative)
-   49 people were incorrectly predicted to be depressed (False
    Positive)
-   10 people were incorrectly predicted to not be depressed (False
    Negative)
-   15 people were correctly predicted to be depressed (True Positive)

## Vocabulary terms {.smaller}

-   True positive ($n_{11}$)
-   True negative ($n_{22}$)
-   False positive, Type I error ($n_{12}$)
-   False negative, Type II error ($n_{21}$)
-   True positive rate (TPR), Recall, Sensitivity, probability of detection, power. P(predicted positive \| total positive) $\frac{\# True Positive}{\#Condition Positive}$
-   True negative rate (TNR), Specificity, selectivity. P(predicted negative \| total negative) $\frac{\# True Negative}{\# Condition Negative}$
-   False positive rate (FPR), fall-out, probability of false alarm. $\frac{\# False Positive}{\# Condition Negative}$
-   False negative rate (FNR), miss rate. $\frac{\# False Negative}{\# Condition Positive}$
-   Prevalence. $\frac{\# Condition Positive}{\# Total Population}$
-   Accuracy. $\frac{\# True Positive + \# True Negative}{\# Total Population}$
-   Balanced Accuracy: $[(n_{11}/n_{.1}) + (n_{22}/n_{.2})]/2$ - Adjusts for class size imbalances
-   Positive Predictive Value (PPV), Precision. P(true positive \| predicted positive) $\frac{\# True Positive}{\# Predicted Condition Positive}$
-   False discovery rate (FDR).$\frac{\# False Positive}{\# Predicted Condition Positive}$
-   False omission rate (FOR).$\frac{\# False Negative}{\# Predicted Condition Negative}$
-   Negative predictive value (NPV).$\frac{\# True Negative}{\# Predicted Condition Negative}$
-   F1 score. The harmonic mean of precision and recall. This ranges from 0 (bad) to 1 (good): $\frac{2 * (Precision * Recall)}{Precision + Recall}$

## ROC Curves

-   ROC curves show the balance between sensitivity and specificity.
-   We'll use the [\[ROCR\]](https://rocr.bioinf.mpi-sb.mpg.de/)
    package. It only takes 3 commands:
    -   calculate `prediction()` using the model
    -   calculate the model `performance()` on both true positive rate and true negative rate for a whole range of cutoff values.
    -   `plot` the curve.
        -   The `colorize` option colors the curve according to the
            probability cutoff point.

```{r}
pr <- prediction(phat.depr, dep_sex_model$y)
perf <- performance(pr, measure="tpr", x.measure="fpr")
plot(perf, colorize=TRUE, lwd=3, print.cutoffs.at=c(seq(0,1,by=0.1)))
abline(a=0, b=1, lty=2)
```

We can also use the `performance()` function to evaluate the $f1$
measure

```{r}
perf.f1 <- performance(pr,measure="f")
perf.acc <- performance(pr,measure="acc")

par(mfrow=c(1,2))
plot(perf.f1)
plot(perf.acc)
```

We can dig into the `perf.f1` object to get the maximum $f1$ value
(`y.value`), then find the row where that value occurs, and link it to the corresponding cutoff value of x.

```{r}
(max.f1 <- max(perf.f1@y.values[[1]], na.rm=TRUE))
(row.with.max <- which(perf.f1@y.values[[1]]==max.f1))
(cutoff.value <- perf.f1@x.values[[1]][row.with.max])
```

A cutoff value of 0.228 provides the most optimal $f1$ score.

## Area under the Curve

-   Can also be used for model comparison: http://yaojenkuo.io/diamondsROC.html
-   The Area under the Curve (auc) also gives you a measure of overall model accuracy.

```{r}
auc <- performance(pr, measure='auc')
auc@y.values
```

## Model Performance

-   Say we decide that a value of 0.22828 is our optimal cutoff value to predict depression using this model. (note here is a GOOD place to use all the decimals.)
-   We can use this probability to classify each row into groups.
    -   The assigned class values must match the data type and levels of the true value.
    -   It also has to be in the same order, so the `0` group needs to come first.
    -   I want this matrix to show up like the one in Wikipedia, so I'm leveraging the forcats package to reverse my factor level ordering.
-   We can calculate a confusion matrix using the similarly named function from the `caret` package.

```{r}
plot.mpp$pred.class2 <- ifelse(plot.mpp$pred.prob <0.22828, 0,1) 
plot.mpp$pred.class2 <- factor(plot.mpp$pred.class2, labels=c("Not Depressed", "Depressed")) %>%   
                        forcats::fct_rev()

confusionMatrix(plot.mpp$pred.class2, forcats::fct_rev(plot.mpp$truth), positive="Depressed")
```

-   192 people were correctly predicted to not be depressed (True
    Negative, $n_{11}$)
-   52 people were incorrectly predicted to be depressed (False
    Positive, $n_{21}$)
-   25 people were incorrectly predicted to not be depressed (False
    Negative, $n_{12}$)
-   25 people were correctly predicted to be depressed (True Positive,
    $n_{22}$)

## Metrics 

-   **Sensitivity/Recall/True positive rate**: `25/(25+25) = .50`
-   **Specificity/true negative rate**: `192/(52+192) = .7869`
-   **Precision/positive predicted value**: `25/(25+52) = .3247`
-   **Accuracy**:: `(25 + 192)/(25+52+25+192) = .7381`
-   **F1 score**: `2*(.3247*.50)/(.3247+.50) = .3937`

