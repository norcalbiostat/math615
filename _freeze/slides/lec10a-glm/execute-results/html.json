{
  "hash": "411459f7822601b122e610a3503f1286",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Generalized Linear Models\"\ndate: \"2025-11-10\"\nauthor: \"Robin Donatello\"\ndescription: \"lec10a\"\nfooter: \"[ðŸ”— https://math615.netlify.app](https://math615.netlify.app)\"\nfrom: markdown+emoji\nformat: \n  revealjs:\n    theme: sky\n    transition: fade\n    slide-number: true\n    incremental: false \nexecute:\n  freeze: auto\n  echo: true\n  message: false\n  warning: false\nknitr:\n  opts_chunk: \n    R.options:\n      width: 200\neditor: \n  markdown: \n    wrap: 72\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Introduction\nOne of the primary assumptions with linear regression, is that the error terms have a specific distribution. Namely: \n\n$$ \\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2}) \\qquad i=1, \\ldots, n, \\quad \\mbox{and } \\epsilon_{i} \\perp \\epsilon_{j}, i \\neq j $$\n\nWhen your outcome variable $y$ is non-continuous/non-normal, the above assumption fails dramatically. \n\n## Generalized Linear Models (GLM) {.smaller}\n\nAllows for different data type outcomes by allowing the linear portion of the model ($\\mathbf{X}\\beta$) to be related to the outcome variable $y$ using a _link_ function, that allows the magnitude of the variance of the errors ($\\sigma$) to be related to the predicted values themselves. \n\nThere are a few overarching types of non-continuous outcomes that can be modeled with GLM's. \n\n* Binary data: Logistic or Probit regression\n* Log-linear models \n* Multinomial/categorical data: Multinomial or Ordinal Logistic regression.  \n* Count data: Poisson regression \n\n\n## Linking the response to the predictors {.smaller}\n\nAll regression models aim to model the expected value of the response variable $Y$ given the observed data $X$, through some link function $C$ \n\n$$E(Y|X) = C(X)$$ \n\nDepending on the data type of $Y$, this link function takes different forms. Examples include: \n\n* Linear regression: C = Identity function (no change)\n* Logistic regression: C = logit function\n* Poisson regression: C = log function\n\n## Fitting GLM\n\nThe general syntax is similar to `lm()`, with the additional required `family=` argument. See `?family` for a list of options. \n\nExample for Logistic regression would be: \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nglm(y ~ x1 + x2 + x3, data=DATA, family=\"binomial\") \n```\n:::\n\n\n\n## Log-linear models {.smaller}\n\nA *log-linear* model is when the log of the response variable is modeled using a linear combination of predictors. \n\n$$ln(Y) \\sim XB +\\epsilon$$\n\n* In statistics, when we refer to the _log_, we mean the natural log _ln_.\n* This type of model is often use to model count data using the Poisson distribution, or to achieve normality when the response variable is right skewed. \n\n## Interpreting results\n\nSince we transformed our outcome before performing the regression, we have to back-transform the coefficient before interpretation. Similar to logistic regression, we need to _exponentiate_ the regression coefficient before interpreting. \n\nWhen using log transformed outcomes, the effect on Y becomes **multiplicative** instead of additive. \n\n* **Additive** For every 1 unit increase in X, y increases by b1\n* **Multiplicative** For every 1 unit increase in X, y is multiplied by $e^{b1}$\n\n## Example\n\nlet $b_{1} = 0.2$. \n\n* **Additive** For every 1 unit increase in X, y increases by 0.2 units.\n* **Multiplicative** For every 1 unit increase in X, y changes by a factor of $e^{0.2} = 1.22$ \n\nSign of coefficient still is directly connected to direction of association. \n\n## Percent Change {#sec-pct-chg .smaller}\n\nThus we interpret the coefficient as a **percentage** change in $Y$ for a unit increase in $x_{j}$. We are interested in the difference between the estimate and the value of 1. \n\n* Let $b_{j} = -0.2$. Then $e^{-0.2} = 0.82$, and $(1-0.82) = .18$\n    * The expected value of $Y$ for when $x=0$ is 18% _lower_ than when $x=1$\n* Let $b_{j} = 0.2$. Then $e^{0.2} = 1.22$, and $(1.22-1) = .22$\n    * The expected value of $Y$ for when $x=0$ is 22% percent _higher_ than when $x=1$\n   \n\n## Example: Personal Income {.smaller} \n\nWe are going to analyze personal income from the AddHealth data set. Income naturally is right skewed, but a log transformation fixes this problem nicely. \n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\"}\n\n```{.r .cell-code}\nggdensity(addhealth, x = \"income\", fill = \"springgreen4\") +\n   stat_overlay_normal_density(color = \"darkgreen\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](lec10a-glm_files/figure-revealjs/unnamed-chunk-3-1.png){width=288}\n:::\n\n```{.r .cell-code}\nggdensity(addhealth, x = \"logincome\", fill = \"skyblue\") +\n   stat_overlay_normal_density(color = \"navy\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](lec10a-glm_files/figure-revealjs/unnamed-chunk-3-2.png){width=288}\n:::\n\n```{.r .cell-code}\nggqqplot(addhealth, x = \"income\")\n```\n\n::: {.cell-output-display}\n![](lec10a-glm_files/figure-revealjs/unnamed-chunk-3-3.png){width=288}\n:::\n\n```{.r .cell-code}\nggqqplot(addhealth, x = \"logincome\")\n```\n\n::: {.cell-output-display}\n![](lec10a-glm_files/figure-revealjs/unnamed-chunk-3-4.png){width=288}\n:::\n:::\n\n\n\n## Identify variables\n\n* Quantitative outcome that has been log transformed: Income (variable `logincome`)\n* Quantitative predictor: typical time waking up on a work day  (variable `wakeup`)\n* Binary predictor: Gender (variable `female_c`)\n \nThe mathematical multivariable model looks like: \n\n$$ln(Y) \\sim \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2}$$\n\n## Fit a linear regression model {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nln.mod.2 <- lm(logincome~wakeup + female_c, data=addhealth)\nsummary(ln.mod.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = logincome ~ wakeup + female_c, data = addhealth)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.32215 -0.33473 -0.00461  0.34058  2.01559 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    10.653062   0.025995 409.805  < 2e-16 ***\nwakeup         -0.014907   0.003218  -4.633 3.73e-06 ***\nfemale_cFemale -0.192710   0.017000 -11.336  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5233 on 3810 degrees of freedom\n  (2691 observations deleted due to missingness)\nMultiple R-squared:  0.03611,\tAdjusted R-squared:  0.0356 \nF-statistic: 71.36 on 2 and 3810 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nThe fitted model is: $ln(\\hat{y}) = 10.65 - 0.0149x_{wakeup} -0.193x_{female}$\n\nThe coefficient estimates $b_{p}$ are not interpreted directly as an effect on $y$. \n\n## Exponentiate & Interpret\n\nRecap: $e^{b_p}$ is the percent change. (ref @sec-pct-chg)\n\n* For every hour later one wakes up in the morning, one can expect to earn `1-exp(-0.015)` = 1.4% less income than someone who wakes up one hour earlier. This is after controlling for gender. \n* Females have on average `1-exp(-0.19)` = 17% percent lower income than males, after controlling for the wake up time. \n\n## All the math at once {.smaller}\n\nExtract point estimate using `coef()`\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nexp(coef(ln.mod.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   (Intercept)         wakeup female_cFemale \n  4.232198e+04   9.852031e-01   8.247215e-01 \n```\n\n\n:::\n:::\n\n\n\nExtract CI using `confint()`\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n1-exp(confint(ln.mod.2)[-1,])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    2.5 %      97.5 %\nwakeup         0.02099299 0.008561652\nfemale_cFemale 0.20231394 0.147326777\n```\n\n\n:::\n:::\n\n\n\nBoth gender and time one wakes up are significantly associated with the amount of personal earnings one makes. Waking up later in the morning is associated with 1.4% (95% CI 0.8%-2%, p<.0001) percent lower income than someone who wakes up one hour earlier. Females have 17% (95% CI 15%-20%, p<.0001) percent lower income than males. ",
    "supporting": [
      "lec10a-glm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}