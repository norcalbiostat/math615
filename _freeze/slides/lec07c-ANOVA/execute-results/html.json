{
  "hash": "a9ced39755bb1c7df61c9d0e4dd47359",
  "result": {
    "markdown": "---\ntitle: \"Inference between multiple means\"\ntitle-slide-attributes:\n    data-background-image: images/plot3norm.png\n    data-background-size: contain\n    data-background-opacity: \"0.3\"\ndate: \"2024-10-16\"\ndescription: \"lec07c\"\nauthor: \"Robin Donatello\"\nfooter: \"[ðŸ”— https://math615.netlify.app](https://math615.netlify.app) / Inference between multiple means\"\nfrom: markdown+emoji\nformat: \n  revealjs:\n    theme: sky\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  message: false\n  warning: false\nknitr:\n  opts_chunk: \n    R.options:\n      width: 200\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n\n# Quantitative Outcome \\~ Categorical Covariate\n\n-   Does knowing what group an observation is in tell you about the\n    value of the response?\n-   Are the means of **two or more** groups are *statistically*\n    different from each other?\n\n## Analysis of Variance (ANOVA) Model\n\n$$\ny_{ij} = \\mu_{j} + \\epsilon_{ij} \\qquad \\qquad\n\\epsilon_{ij} \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^{2})\n$$\n\n-   Response data $y_{ij}$ from observation $i=1\\ldots n$ belonging to\n    group $j=1,2, \\ldots J$\n-   The random error terms $\\epsilon_{ij}$ are independently and\n    identically distributed (iid) as normal with mean zero and common\n    variance.\n-   Look familiar? T-test is a special case of ANOVA.\n\n## Hypothesis specification\n\nThe null hypothesis is that there is no difference in the mean of the\nquantitative variable across groups (categorical variable), while the\nalternative is that there is a difference.\n\n-   $H_0$: The mean outcome is the same across all groups.\n    $\\mu_1 = \\mu_2 = \\cdots = \\mu_j$\n-   $H_A$: At least one mean is different.\n\n## Multiple Comparison {background-image=\"https://imgs.xkcd.com/comics/significant.png\" background-size=\"300px\" background-position=\"85% 50%\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**Why not multiple T-tests between all pairs of groups?**\n\n\\\n\nEach time you conduct a test, you risk coming to the wrong conclusion.\n\nRepeated tests compound that chance of being wrong.\n\n\\\n\nImg Ref: <https://xkcd.com/882>\n:::\n\n::: {.column width=\"40%\"}\n:::\n:::\n\n## Proof\n\n-   Set a Type 1 error rate of $\\alpha = .05$ (significant results due\n    to randomness)\n-   Conduct 10 analyses on the same data, but on different subsets.\n-   Chance of making an error on any one test: .05\n-   Chance of NOT making an error on any one test: .95\n-   Chance of NOT making an error on ALL 10 tests: $.95^{10} = .6$\n-   --> 40% chance of making a Type 1 error on *at least one* test.\n\n## Visual Comparison\n\nCompare groups I, II, and III. Can you visually determine if the\ndifferences in the group centers is due to chance or not? What about\ngroups IV, V, and VI?\n\n![Side-by-side dot plot for the outcomes for six\ngroups.](images/toyANOVA.png){fig-align=\"center\"}\n\n## Analysis of Variance\n\nThe total amount of variation in our quantitative outcome can be broken into two parts: \n\n> Total Variation = Between Group Variation (model) + Within Group Variation (residual)\n\n* The portion of the variance in the outcome that is explained by the groups (model)\n* The portion that's leftover due to unexplained randomness (residual) \n\nBy looking at a ratio of these variance portions, we can determine if the variation observed is due to the groups, or random chance. \n\n## Sum of Squares {.smaller}\n\nVariation is measured using the Sum of Squares (SS): The sum of the\nsquares within a group (SSE), the sum of squares between groups (SSG),\nand the total sum of squares (SST).\n\n$$\nSST =  \\sum_{i=1}^{I}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}..)^{2} = (N-1)Var(Y)\n$$\n\n**SST (Total)**: Measures the variation of the $N$ data points around\nthe overall mean.\n\n## Sum of Squares {.smaller}\n\n$$\nSSG = \\sum_{i=1}^{I}n_{i}(\\bar{y}_{i.}-\\bar{y}..)^{2} = n_{1}(\\bar{y}_{1.}-\\bar{y}..)^{2} + n_{2}(\\bar{y}_{2.}-\\bar{y}..)^{2} + \\ldots + n_{I}(\\bar{y}_{I.}-\\bar{y}..)^{2}\n$$\n\n**SSG (Between groups)**: Measures the variation of the $I$ group means\naround the overall mean.\n\n$$\nSSE = \\sum_{i=1}^{I}\\sum_{j=1}^{n_{i}}(y_{ij}-\\bar{y}_{i.})^{2} = \\sum_{i=1}^{I}(n_{i}-1)Var(Y_{i})\n$$\n\n**SSE (Within group)**: Measures the variation of each observation\naround its group mean.\n\n## Analysis of Variance Table\n\nThe results are typically summarized in an ANOVA table.\n\n| Source    | SS      | df    | MS                      | F                 |\n|-----------|---------|-------|-------------------------|-------------------|\n| Groups    | SSG     | $I-1$ | MSG = $\\frac{SSG}{I-1}$ | $\\frac{MSG}{MSE}$ |\n| Error     | SSE     | $N-I$ | MSE = $\\frac{MSE}{N-I}$ |                   |\n| **Total** | **SST** | $N-1$ |                         |                   |\n\nThe value in the **F** column is the test statistic, and has a F\ndistribution with degrees of freedom (df) dependent on the number of\ngroups (I-1), and the number of observations (N-I).\n\n## The F-distribution\n\nThe $p$-value is the **area to the right** of the F statistic density\ncurve. This is always to the right because the F-distribution is\ntruncated at 0 and skewed right. This is true regardless of the $df$.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf1 <- c(3, 5, 8)\ndf2 <- c(4, 6, 10)\n\nplot(NULL, xlim = c(0, 5), ylim = c(0, 1), xlab = expression(F), ylab=\"\", main = \"F Distribution\", axes=FALSE)\naxis(2, labels = FALSE, tick = FALSE)\naxis(1); box()\nfor (i in 1:3) {\n  curve(df(x, df1[i], df2[i]), from = 0, to = 5, col = i, add = TRUE, lty = i, lwd=2)\n}\nlegend(\"topright\", legend = paste(\"df1 =\", df1, \", df2 =\", df2), col = 1:3, lty = 1:3)\n```\n\n::: {.cell-output-display}\n![](lec07c-ANOVA_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Assumptions\n\nGenerally we must check three conditions on the data before performing\nANOVA:\n\n-   The observations are independent within and across groups\n-   The data within each group are nearly normal\n-   The variability across the groups is about equal.\n\n## Example: March of the Penguins\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npen <- palmerpenguins::penguins\n```\n:::\n\n\n\n### 1. Identify response and explanatory variables\n\n-   Species = categorical explanatory variable\n-   Flipper length = quantitative response variable\n\n## 2. Visualize and summarise {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggdist) # for the \"half-violin\" plot (stat_slab)\nggplot(pen, aes(x=flipper_length_mm, y=species, fill=species)) + \n      stat_slab(alpha=.5, justification = 0) + \n      geom_boxplot(width = .2,  outlier.shape = NA) + \n      geom_jitter(alpha = 0.5, height = 0.05) +\n      stat_summary(fun=\"mean\", geom=\"point\", col=\"red\", size=4, pch=17) + \n      theme_bw() + \n      labs(x=\"Flipper Length (mm)\", y = \"Species\") + \n      theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](lec07c-ANOVA_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npen %>% group_by(species) %>% \n  summarise(mean=mean(flipper_length_mm, na.rm=TRUE), \n            sd = sd(flipper_length_mm, na.rm=TRUE), \n            IQR = IQR(flipper_length_mm, na.rm=TRUE)) %>%\n  kable(digits = 1)\n```\n\n::: {.cell-output-display}\n|species   |  mean|  sd| IQR|\n|:---------|-----:|---:|---:|\n|Adelie    | 190.0| 6.5|   9|\n|Chinstrap | 195.8| 7.1|  10|\n|Gentoo    | 217.2| 6.5|   9|\n:::\n:::\n\n\n:::\n:::\n\nThe distribution of flipper length varies across the species, with\nGentoo having the largest flippers on average at 217.2mm compared to\nAdelie (190mm) and Chinstrap (195.8mm). The distributions are normally\ndistributed with very similar spreads, Chinstrap has the most variable\nflipper length with a SD of 7.1 compared to 6.5 for the other two\nspecies.\n\n<small> [*A blog post about raincloud plots vs\nviolins*](https://www.cedricscherer.com/2021/06/06/visualizing-distributions-with-raincloud-plots-and-how-to-create-them-with-ggplot2/)\n</small>\n\n## 3. Write the null and research hypothesis in words and symbols. {.smaller}\n\n-   Null Hypothesis: There is no association between flipper length and\n    species.\n-   Alternate Hypothesis: There is an association between flipper length\n    and species.\n\nLet $\\mu_{A}, \\mu_{C}$ and $\\mu_{G}$ be the average flipper length for\nthe *Adelie, Chinstrap* and *Gentoo* species of penguins respectively.\n\n\\\n\n$H_{0}: \\mu_{A} = \\mu_{C} = \\mu_{G}$ \n\n$H_{A}:$ At least one $\\mu_{j}$ is different.\n\n## 4. State and justify the analysis model. Check assumptions. {.smaller}\n\nWe are comparing means from multiple groups, so an ANOVA is the\nappropriate procedure. We need to check for independence, approximate\nnormality and approximately equal variances across groups.\n\n**Independence**: We are assuming that each penguin was sampled\nindependently of each other, and that the species themselves are\nindependent of each other.\n\n**Normality**: The distributions of flipper length within each group are\nfairly normal\n\n**Equal variances**: Both the standard deviation and IQR (as measures of\nvariability) are very similar across all groups.\n\n## 5. Conduct the test and make a decision about the plausibility of the alternative hypothesis. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naov(flipper_length_mm ~ species, data=pen) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \nspecies       2  52473   26237   594.8 <2e-16 ***\nResiduals   339  14953      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\n```\n:::\n:::\n\n\n\nThe pvalue is very small, so there is evidence to support $H_{a}$: at\nleast one mean is different.\n\n## 6. Write a conclusion in context of the problem. Include the point estimates, confidence interval for the difference and p-value.\n\nThere is sufficient evidence to believe that the average flipper length\nis significantly different between the Adelie, Chinstrap and Gentoo\nspecies of penguins (p\\<.0001).\n\n## Multiple / Post-Hoc comparisons: Which group is different? {.smaller}\n\n-   Only appropriate  if your ANOVA is significant.\n-   The overall ANOVA can be significant and NOT have any significant\n    differences when you look at the post hoc results. The reason is\n    that the two analyses ask two different questions.\n    -   The ANOVA is testing the overall pattern of the data and asking\n        if as a whole the explanatory variable has a relationship (or\n        lack thereof) with the response variable.\n    -   The post hoc is asking if one level of the explanatory variable\n        is significantly different than another for the response\n        variable. The post hoc is not as sensitive to differences as the\n        ANOVA.\n    -   **The family-wise error rate of** $\\alpha$ is maintained\n-   Differences in group means can be non-significant at the post hoc\n    level, but significant at the ANOVA level.\n\n## Tukey HSD {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTukeyHSD(aov(flipper_length_mm ~ species, data=pen))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = flipper_length_mm ~ species, data = pen)\n\n$species\n                      diff       lwr       upr p adj\nChinstrap-Adelie  5.869887  3.586583  8.153191     0\nGentoo-Adelie    27.233349 25.334376 29.132323     0\nGentoo-Chinstrap 21.363462 19.000841 23.726084     0\n```\n:::\n:::\n\n\n\nThe results of the Tukey HSD post-hoc test indicate that the average\nflipper length in mm is significantly different between all pairs of\npenguin species at the 5% significance level. Chinstrap has an average\nflipper length 5.87mm(3.59-8.15) larger than Adelie, whereas Gentoo has\nan average flipper length 27.23mm(25.33-29.13) larger than Adelie.\n\n## Coefficient of Determination $R^{2} = \\frac{SSG}{SST}$ {.smaller}\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \nspecies       2  52473   26237   594.8 <2e-16 ***\nResiduals   339  14953      44                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\n52473/(52473 + 14953)*100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 77.82309\n```\n:::\n:::\n\n\n\nThe coefficient of determination is interpreted as the % of the\nvariation seen in the outcome that is due to subject level variation\nwithin each of the treatment groups. The strength of this measure can be\nthought of in a similar manner as the correlation coefficient $\\rho$,\n$<.3$ indicates a poor fit, $<.5$ indicates a medium fit, and $>.7$\nindicates a good fit.\n\n> 77.8% of the variation in flipper length can be explained by the\n> species of penguin\n\n## Non-parametric tests\n\n-   Many stat tests rely on assumptions that ensure the sample estimate\n    can be modeled with a normal distribution.\n-   What do you do if your assumptions aren't met?\n-   We can \"relax\" some of those assumptions and perform a more robust,\n    but less powerful test.\n    -   less power means you need more data to draw a conclusion with\n        the same amount of confidence.\n-   No detailed examples will be provided in these notes. You tend to\n    learn/use these on an \"as-needed\" basis.\n\n## Kruskal-Wallis\n\n-   The\n    [Kruskal-Wallis](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance)\n    test is the most common **non-parametric** method for testing\n    whether or not groups of observations come from the same overall\n    distribution.\n-   By comparing the *medians* instead of the means, we can remove the\n    normality assumption on the residuals.\n-   Null hypothesis is now that the medians of all groups are equal vs\n    at least one population median is different.\n\n## K-W and all pairwise Wilcox Tests {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkruskal.test(flipper_length_mm ~ species, data = pen)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  flipper_length_mm by species\nKruskal-Wallis chi-squared = 244.89, df = 2, p-value < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\npairwise.wilcox.test(pen$flipper_length_mm, pen$species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  pen$flipper_length_mm and pen$species \n\n          Adelie Chinstrap\nChinstrap 3e-08  -        \nGentoo    <2e-16 <2e-16   \n\nP value adjustment method: holm \n```\n:::\n:::\n\n\n\nIn this case, the non-parametric tests come to the same conclusion as the parametric ones. If there is a discrepency, go with the non-parametric test since it has fewer assumptions. \n",
    "supporting": [
      "lec07c-ANOVA_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}